{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSC 180 Intelligent Systems \n",
    "\n",
    "#### William Lorence, Ajaydeep Singh, Romin Akoliya, Abdurraziq Paikur\n",
    "\n",
    "#### California State University, Sacramento\n",
    "\n",
    "# Final Project: NBA Outcome Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nba_api.stats.static import teams, players\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Team and Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SEASON_ID     TEAM_ID TEAM_ABBREVIATION      TEAM_NAME     GAME_ID  \\\n",
      "0     22024  1610612737               ATL  Atlanta Hawks  0022400258   \n",
      "1     22024  1610612737               ATL  Atlanta Hawks  0022400250   \n",
      "2     22024  1610612737               ATL  Atlanta Hawks  0022400239   \n",
      "3     22024  1610612737               ATL  Atlanta Hawks  0022400012   \n",
      "4     22024  1610612737               ATL  Atlanta Hawks  0022400001   \n",
      "\n",
      "    GAME_DATE      MATCHUP WL  MIN  PTS  ...  FT_PCT  OREB  DREB  REB  AST  \\\n",
      "0  2024-11-20    ATL @ GSW  L  238   97  ...   0.625    21    44   65   27   \n",
      "1  2024-11-18    ATL @ SAC  W  239  109  ...   0.833     6    31   37   34   \n",
      "2  2024-11-17    ATL @ POR  L  240  110  ...   0.840    16    31   47   30   \n",
      "3  2024-11-15  ATL vs. WAS  W  240  129  ...   0.833    15    40   55   28   \n",
      "4  2024-11-12    ATL @ BOS  W  240  117  ...   0.538    19    25   44   35   \n",
      "\n",
      "    STL  BLK  TOV  PF  PLUS_MINUS  \n",
      "0   9.0    5   17  17       -23.0  \n",
      "1   7.0    7   14  19         1.0  \n",
      "2  10.0    7   25  19        -4.0  \n",
      "3  10.0   10   16  17        12.0  \n",
      "4  16.0    2   16  17         1.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "\n",
    "# Example: Get all games played by the Atlanta Hawks\n",
    "gamefinder = leaguegamefinder.LeagueGameFinder(team_id_nullable=1610612737)\n",
    "games = gamefinder.get_data_frames()[0]  # Fetch the data as a DataFrame\n",
    "\n",
    "# Display the first few rows\n",
    "print(games.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Format Data: Extract key information such as:\n",
    "\n",
    "### Season\n",
    "### Wins and losses\n",
    "### Opponent teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON      TEAM_NAME   GAME_DATE      MATCHUP    WIN\n",
      "0    2202  Atlanta Hawks  2024-11-20    ATL @ GSW  False\n",
      "1    2202  Atlanta Hawks  2024-11-18    ATL @ SAC   True\n",
      "2    2202  Atlanta Hawks  2024-11-17    ATL @ POR  False\n",
      "3    2202  Atlanta Hawks  2024-11-15  ATL vs. WAS   True\n",
      "4    2202  Atlanta Hawks  2024-11-12    ATL @ BOS   True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/3731708797.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_games['WIN'] = recent_games['WL'] == 'W'\n"
     ]
    }
   ],
   "source": [
    "# Filter for games from the last 20 years\n",
    "games['SEASON'] = games['SEASON_ID'].str[:4].astype(int)\n",
    "recent_games = games[games['SEASON'] >= 2004]\n",
    "\n",
    "# Create a simple win/loss indicator\n",
    "recent_games['WIN'] = recent_games['WL'] == 'W'\n",
    "\n",
    "# Display processed data\n",
    "print(recent_games[['SEASON', 'TEAM_NAME', 'GAME_DATE', 'MATCHUP', 'WIN']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Player Career Averages: Use the playercareerstats endpoint to get data for individual players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON_ID   PTS  AST  REB\n",
      "17   2020-21  1126  350  346\n",
      "18   2021-22  1695  349  459\n",
      "19   2022-23  1590  375  457\n",
      "20   2023-24  1822  589  518\n",
      "21   2024-25   329  132  114\n"
     ]
    }
   ],
   "source": [
    "from nba_api.stats.endpoints import playercareerstats\n",
    "\n",
    "# Example: Fetch career stats for a specific player (LeBron James)\n",
    "player_id = 2544  # LeBron James' ID\n",
    "career = playercareerstats.PlayerCareerStats(player_id=player_id)\n",
    "career_data = career.get_data_frames()[0]\n",
    "\n",
    "# Display the career averages\n",
    "print(career_data[['SEASON_ID', 'PTS', 'AST', 'REB']].tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data: Save the data into CSV files for easier use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_games.to_csv('team_records.csv', index=False)\n",
    "career_data.to_csv('player_career_averages.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Combining Datasets\n",
    "### Load the Datasets: Import the CSV files into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team Data:\n",
      "   SEASON_ID     TEAM_ID TEAM_ABBREVIATION      TEAM_NAME   GAME_ID  \\\n",
      "0      22024  1610612737               ATL  Atlanta Hawks  22400258   \n",
      "1      22024  1610612737               ATL  Atlanta Hawks  22400250   \n",
      "2      22024  1610612737               ATL  Atlanta Hawks  22400239   \n",
      "3      22024  1610612737               ATL  Atlanta Hawks  22400012   \n",
      "4      22024  1610612737               ATL  Atlanta Hawks  22400001   \n",
      "\n",
      "    GAME_DATE      MATCHUP WL  MIN  PTS  ...  DREB  REB  AST   STL  BLK  TOV  \\\n",
      "0  2024-11-20    ATL @ GSW  L  238   97  ...    44   65   27   9.0    5   17   \n",
      "1  2024-11-18    ATL @ SAC  W  239  109  ...    31   37   34   7.0    7   14   \n",
      "2  2024-11-17    ATL @ POR  L  240  110  ...    31   47   30  10.0    7   25   \n",
      "3  2024-11-15  ATL vs. WAS  W  240  129  ...    40   55   28  10.0   10   16   \n",
      "4  2024-11-12    ATL @ BOS  W  240  117  ...    25   44   35  16.0    2   16   \n",
      "\n",
      "   PF  PLUS_MINUS  SEASON    WIN  \n",
      "0  17       -23.0    2202  False  \n",
      "1  19         1.0    2202   True  \n",
      "2  19        -4.0    2202  False  \n",
      "3  17        12.0    2202   True  \n",
      "4  17         1.0    2202   True  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Player Data:\n",
      "   PLAYER_ID SEASON_ID  LEAGUE_ID     TEAM_ID TEAM_ABBREVIATION  PLAYER_AGE  \\\n",
      "0       2544   2003-04          0  1610612739               CLE        19.0   \n",
      "1       2544   2004-05          0  1610612739               CLE        20.0   \n",
      "2       2544   2005-06          0  1610612739               CLE        21.0   \n",
      "3       2544   2006-07          0  1610612739               CLE        22.0   \n",
      "4       2544   2007-08          0  1610612739               CLE        23.0   \n",
      "\n",
      "   GP  GS     MIN  FGM  ...  FT_PCT  OREB  DREB  REB  AST  STL  BLK  TOV   PF  \\\n",
      "0  79  79  3120.0  622  ...   0.754    99   333  432  465  130   58  273  149   \n",
      "1  80  80  3388.0  795  ...   0.750   111   477  588  577  177   52  262  146   \n",
      "2  79  79  3361.0  875  ...   0.738    75   481  556  521  123   66  260  181   \n",
      "3  78  78  3190.0  772  ...   0.698    83   443  526  470  125   55  250  171   \n",
      "4  75  74  3027.0  794  ...   0.712   133   459  592  539  138   81  255  165   \n",
      "\n",
      "    PTS  \n",
      "0  1654  \n",
      "1  2175  \n",
      "2  2478  \n",
      "3  2132  \n",
      "4  2250  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "team_data = pd.read_csv('team_records.csv')\n",
    "player_data = pd.read_csv('player_career_averages.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Team Data:\")\n",
    "print(team_data.head())\n",
    "print(\"\\nPlayer Data:\")\n",
    "print(player_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Team Data:\n",
    "\n",
    "#### Extract relevant columns (e.g., season, team name, win/loss).\n",
    "#### Encode the target variable (win/loss) as 1 for win and 0 for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON      TEAM_NAME   GAME_DATE      MATCHUP  WIN\n",
      "0    2202  Atlanta Hawks  2024-11-20    ATL @ GSW    0\n",
      "1    2202  Atlanta Hawks  2024-11-18    ATL @ SAC    1\n",
      "2    2202  Atlanta Hawks  2024-11-17    ATL @ POR    0\n",
      "3    2202  Atlanta Hawks  2024-11-15  ATL vs. WAS    1\n",
      "4    2202  Atlanta Hawks  2024-11-12    ATL @ BOS    1\n"
     ]
    }
   ],
   "source": [
    "team_data['WIN'] = team_data['WIN'].astype(int)  # Convert Boolean to integer\n",
    "team_data_processed = team_data[['SEASON', 'TEAM_NAME', 'GAME_DATE', 'MATCHUP', 'WIN']]\n",
    "print(team_data_processed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Player Data:\n",
    "\n",
    "#### Filter career averages for relevant stats (e.g., points, assists, rebounds).\n",
    "#### Create a dictionary of player stats grouped by season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON_ID  PLAYER_ID   PTS  AST  REB\n",
      "0       2003       2544  1654  465  432\n",
      "1       2004       2544  2175  577  588\n",
      "2       2005       2544  2478  521  556\n",
      "3       2006       2544  2132  470  526\n",
      "4       2007       2544  2250  539  592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/2326349354.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  player_data_filtered['SEASON_ID'] = player_data_filtered['SEASON_ID'].str[:4].astype(int)  # Extract season year\n"
     ]
    }
   ],
   "source": [
    "player_data_filtered = player_data[['SEASON_ID', 'PLAYER_ID', 'PTS', 'AST', 'REB']]\n",
    "player_data_filtered['SEASON_ID'] = player_data_filtered['SEASON_ID'].str[:4].astype(int)  # Extract season year\n",
    "print(player_data_filtered.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON      TEAM_NAME   GAME_DATE      MATCHUP  WIN  TEAM_PTS_AVG  \\\n",
      "0    2202  Atlanta Hawks  2024-11-20    ATL @ GSW    0           NaN   \n",
      "1    2202  Atlanta Hawks  2024-11-18    ATL @ SAC    1           NaN   \n",
      "2    2202  Atlanta Hawks  2024-11-17    ATL @ POR    0           NaN   \n",
      "3    2202  Atlanta Hawks  2024-11-15  ATL vs. WAS    1           NaN   \n",
      "4    2202  Atlanta Hawks  2024-11-12    ATL @ BOS    1           NaN   \n",
      "\n",
      "   TEAM_AST_AVG  TEAM_REB_AVG  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/110984493.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG']] = team_data_processed.apply(\n",
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/110984493.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG']] = team_data_processed.apply(\n",
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/110984493.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG']] = team_data_processed.apply(\n"
     ]
    }
   ],
   "source": [
    "def calculate_team_stats(season, team_name, player_data):\n",
    "    # Filter players for the given team and season\n",
    "    players_on_team = player_data[player_data['SEASON_ID'] == season]\n",
    "    \n",
    "    # Calculate averages for relevant stats\n",
    "    avg_pts = players_on_team['PTS'].mean()\n",
    "    avg_ast = players_on_team['AST'].mean()\n",
    "    avg_reb = players_on_team['REB'].mean()\n",
    "    \n",
    "    return avg_pts, avg_ast, avg_reb\n",
    "\n",
    "# Add separate columns for each stat\n",
    "team_data_processed[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG']] = team_data_processed.apply(\n",
    "    lambda row: pd.Series(calculate_team_stats(row['SEASON'], row['TEAM_NAME'], player_data_filtered)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(team_data_processed.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data_processed.to_csv('combined_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['combined_dataset.csv', 'team_records.csv', 'finalproject.ipynb', '.ipynb_checkpoints', 'enhanced_dataset.csv', 'player_career_averages.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (2520, 3)\n",
      "Test features shape: (1080, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('combined_dataset.csv')\n",
    "\n",
    "# Select features and target\n",
    "X = data[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG']]  # Use more features if available\n",
    "y = data['WIN']  # Target variable\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shape of training and test sets\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Test features shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                256       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2369 (9.25 KB)\n",
      "Trainable params: 2369 (9.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    Dense(32, activation='relu'),  # Hidden layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 803us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 770us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 798us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 808us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping to monitor validation loss\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,  # 20% of training data for validation\n",
    "    epochs=50,             # Maximum number of epochs\n",
    "    batch_size=32,         # Number of samples per gradient update\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1              # Display training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 440us/step - loss: nan - accuracy: 0.5185\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.5185185074806213\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 403us/step\n",
      "Confusion Matrix:\n",
      "[[560   0]\n",
      " [520   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.68       560\n",
      "           1       0.00      0.00      0.00       520\n",
      "\n",
      "    accuracy                           0.52      1080\n",
      "   macro avg       0.26      0.50      0.34      1080\n",
      "weighted avg       0.27      0.52      0.35      1080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/2731300222.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']] = team_data_processed.apply(\n",
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/2731300222.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']] = team_data_processed.apply(\n",
      "/var/folders/z2/ns_w84j13gdbd48gnvk2np5c0000gn/T/ipykernel_52588/2731300222.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_data_processed[['OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']] = team_data_processed.apply(\n"
     ]
    }
   ],
   "source": [
    "# Merge opponent stats into the dataset\n",
    "def calculate_opponent_stats(row, player_data):\n",
    "    # Extract season and opponent team name from the row\n",
    "    season = row['SEASON']\n",
    "    opponent = row['MATCHUP'].split(' ')[-1]\n",
    "    \n",
    "    # Filter player data for the opponent team and season\n",
    "    opponent_players = player_data[player_data['SEASON_ID'] == season]\n",
    "    avg_pts = opponent_players['PTS'].mean()\n",
    "    avg_ast = opponent_players['AST'].mean()\n",
    "    avg_reb = opponent_players['REB'].mean()\n",
    "    \n",
    "    return avg_pts, avg_ast, avg_reb\n",
    "\n",
    "# Add opponent stats\n",
    "team_data_processed[['OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']] = team_data_processed.apply(\n",
    "    lambda row: pd.Series(calculate_opponent_stats(row, player_data_filtered)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated dataset\n",
    "team_data_processed.to_csv('enhanced_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 961us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 938us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 982us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 970us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 935us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 945us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 926us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 944us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 924us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 939us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 978us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 965us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 939us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 923us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 943us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 911us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 938us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 988us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 966us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 932us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 957us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 957us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 963us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 950us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 920us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 917us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 921us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 932us/step - loss: nan - accuracy: 0.4995 - val_loss: nan - val_accuracy: 0.5317\n"
     ]
    }
   ],
   "source": [
    "# Load the enhanced dataset\n",
    "data = pd.read_csv('enhanced_dataset.csv')\n",
    "\n",
    "# Include opponent stats in features\n",
    "X = data[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG', 'OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']]\n",
    "y = data['WIN']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Retrain the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 441us/step - loss: nan - accuracy: 0.5185\n",
      "Improved Test Loss: nan\n",
      "Improved Test Accuracy: 0.5185185074806213\n",
      "34/34 [==============================] - 0s 423us/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[560   0]\n",
      " [520   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.68       560\n",
      "           1       0.00      0.00      0.00       520\n",
      "\n",
      "    accuracy                           0.52      1080\n",
      "   macro avg       0.26      0.50      0.34      1080\n",
      "weighted avg       0.27      0.52      0.35      1080\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the improved model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Improved Test Loss: {test_loss}\")\n",
    "print(f\"Improved Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIN\n",
      "0    1835\n",
      "1    1765\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(data['WIN'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding a home game indicator\n",
    "data['HOME_GAME'] = data['MATCHUP'].apply(lambda x: 1 if 'vs.' in x else 0)\n",
    "\n",
    "# Example: Adding recent form (requires sorting by game date)\n",
    "data['RECENT_WINS'] = data.groupby('TEAM_NAME')['WIN'].rolling(5).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Drop NaN values generated from rolling averages\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust the Model Archtecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TEAM_PTS_AVG  TEAM_AST_AVG  TEAM_REB_AVG  OPP_PTS_AVG  OPP_AST_AVG  \\\n",
      "0           NaN           NaN           NaN          NaN          NaN   \n",
      "1           NaN           NaN           NaN          NaN          NaN   \n",
      "2           NaN           NaN           NaN          NaN          NaN   \n",
      "3           NaN           NaN           NaN          NaN          NaN   \n",
      "4           NaN           NaN           NaN          NaN          NaN   \n",
      "\n",
      "   OPP_REB_AVG  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "Feature Set Shape: (3600, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.head())  # Display the first few rows\n",
    "print(\"Feature Set Shape:\", X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "Name: WIN, dtype: int64\n",
      "Target Shape: (3600,)\n"
     ]
    }
   ],
   "source": [
    "print(y.head())  # Display the first few rows\n",
    "print(\"Target Shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   TEAM_PTS_AVG  0 non-null      float64\n",
      " 1   TEAM_AST_AVG  0 non-null      float64\n",
      " 2   TEAM_REB_AVG  0 non-null      float64\n",
      " 3   OPP_PTS_AVG   0 non-null      float64\n",
      " 4   OPP_AST_AVG   0 non-null      float64\n",
      " 5   OPP_REB_AVG   0 non-null      float64\n",
      " 6   HOME_GAME     0 non-null      int64  \n",
      " 7   RECENT_WINS   0 non-null      float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 0.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data[['TEAM_PTS_AVG', 'TEAM_AST_AVG', 'TEAM_REB_AVG', 'OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG', 'HOME_GAME', 'RECENT_WINS']].info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OPP_PTS_AVG  OPP_AST_AVG  OPP_REB_AVG\n",
      "0          NaN          NaN          NaN\n",
      "1          NaN          NaN          NaN\n",
      "2          NaN          NaN          NaN\n",
      "3          NaN          NaN          NaN\n",
      "4          NaN          NaN          NaN\n"
     ]
    }
   ],
   "source": [
    "# Check if opponent stats are being calculated correctly\n",
    "print(team_data_processed[['OPP_PTS_AVG', 'OPP_AST_AVG', 'OPP_REB_AVG']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_opponent_stats(row, player_data):\n",
    "    # Extract season and opponent team name from the row\n",
    "    season = row['SEASON']\n",
    "    opponent = row['MATCHUP'].split(' ')[-1]  # Adjust this based on actual MATCHUP format\n",
    "\n",
    "    # Filter player data for the opponent team and season\n",
    "    opponent_players = player_data[(player_data['SEASON_ID'] == season) & (player_data['TEAM_NAME'] == opponent)]\n",
    "    \n",
    "    if not opponent_players.empty:\n",
    "        # Calculate average stats for the opponent team\n",
    "        avg_pts = opponent_players['PTS'].mean()\n",
    "        avg_ast = opponent_players['AST'].mean()\n",
    "        avg_reb = opponent_players['REB'].mean()\n",
    "    else:\n",
    "        # Default values for missing opponent data\n",
    "        avg_pts = 0\n",
    "        avg_ast = 0\n",
    "        avg_reb = 0\n",
    "    \n",
    "    return avg_pts, avg_ast, avg_reb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 6, 1)]            0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 4, 128)            512       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 2, 128)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 128)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17025 (66.50 KB)\n",
      "Trainable params: 17025 (66.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the CNN model using \"sample\" structure\n",
    "visible = Input(shape=(X_train.shape[1], 1))\n",
    "conv1 = Conv1D(128, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "drop1 = Dropout(0.2)(pool1)\n",
    "flat = Flatten()(drop1)\n",
    "\n",
    "# Interpretation model\n",
    "dense1 = Dense(64, activation='relu')(flat)\n",
    "drop2 = Dropout(0.2)(dense1)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "drop3 = Dropout(0.2)(dense2)\n",
    "output = Dense(1, activation='sigmoid')(drop2)  # Output layer + sigmoid for binary classification\n",
    "\n",
    "# Model creation\n",
    "model_cnn = Model(inputs=visible, outputs=output)\n",
    " \n",
    "# Summarize layers\n",
    "print(model_cnn.summary())\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Input X contains NaN values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_cnn_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check before training to ensure there arent any missing or Nan values\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(X)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X contains NaN values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(y)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget y contains NaN values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input X contains NaN values."
     ]
    }
   ],
   "source": [
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_cnn_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Check before training to ensure there arent any missing or Nan values\n",
    "assert not np.any(np.isnan(X)), \"Input X contains NaN values.\"\n",
    "assert not np.any(np.isnan(y)), \"Target y contains NaN values.\"\n",
    "\n",
    "# Train the model\n",
    "history = model_cnn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print training completion\n",
    "print(\"CNN model training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
